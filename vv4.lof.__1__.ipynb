{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "result_dir = 'result'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = 'vv'\n",
    "pf = '__1__'\n",
    "\n",
    "Xval = np.load(f'{data_dir}/{af}3.v.X.{pf}.npy', allow_pickle=True)\n",
    "y_val = np.load(f'{data_dir}/{af}3.v.y.npy', allow_pickle=True)\n",
    "\n",
    "#? features names\n",
    "fts_names = [line.strip() for line in open(f'{data_dir}/{af}3.fts_cols.{pf}.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" split train test \"\"\"\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Xtrain, Xtest, y_train, y_test = train_test_split(Xval, y_val, test_size=0.3, random_state=42)\n",
    "\n",
    "# print(Xtrain.shape, Xtest.shape)\n",
    "# print(len(np.where(y_train == 1)[0]), len(np.where(y_test == 1)[0]))\n",
    "\n",
    "# # print(list(np.where(y_train == 1)[0]))\n",
    "# # print(list(np.where(y_test == 1)[0]))\n",
    "# print(np.where(y_train == 1)[0])\n",
    "# print(np.where(y_test == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Xtrain = Xtrain[100058:,:]\n",
    "# Xtrain = Xtrain[300000:,:]\n",
    "# Xtest = Xtest[100100:,:]\n",
    "# y_train = y_train[300000:]\n",
    "# y_test = y_test[100100:]\n",
    "\n",
    "# print(Xtrain.shape, Xtest.shape)\n",
    "# print(len(np.where(y_train == 1)[0]), len(np.where(y_test == 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(np.where(y_val == 1)[0]))\n",
    "# print(np.where(y_val == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval = Xval[462995:,:]\n",
    "y = y_val[462995:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(fts_names)):\n",
    "#     print(i, fts_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, PrecisionRecallDisplay, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    #? relabel to use classification metrics\n",
    "    y_pred[y_pred != -1] = 0\n",
    "    y_pred[y_pred == -1] = 1\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    print(roc_auc)\n",
    "\n",
    "    # display = PrecisionRecallDisplay.from_predictions(y_true, y_pred)\n",
    "    # _ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_result(model, X, y_pred, scores, model_name, selected_fts_names, roc_auc, expname='', config=None):\n",
    "    ra = '{:.2f}'.format(round(roc_auc * 100, 2))\n",
    "    # if ra <= 70: #? not good enough to even bother\n",
    "    #     return\n",
    "    \n",
    "    #? save model\n",
    "    pickle.dump(model, open(f'{result_dir}/{af}4.{pf}.{model_name}.{expname}.__{ra}__.model.pkl','wb'))\n",
    "\n",
    "    #? save prediction\n",
    "    np.save(f'{result_dir}/{af}4.{pf}.{model_name}.{expname}.__{ra}__.result.t.y_pred.npy', y_pred)\n",
    "    np.save(f'{result_dir}/{af}4.{pf}.{model_name}.{expname}.result.__{ra}__.t.scores.npy', scores)\n",
    "\n",
    "    #? save transformed X as well\n",
    "    np.save(f'{result_dir}/{af}4.{pf}.{model_name}.{expname}.__{ra}__.data.X.npy', X)\n",
    "\n",
    "    #? save fts\n",
    "    open(f'{result_dir}/{af}4.{pf}.{model_name}.{expname}.__{ra}__.fts.txt', 'w').write('\\n'.join(selected_fts_names))\n",
    "    \n",
    "    #? save config\n",
    "    if config is not None:\n",
    "        open(f'{result_dir}/{af}4.{pf}.{model_name}.{expname}.__{ra}__.config.txt', 'w').write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def test_param(x, y, n_neighbors, contamination, metric='minkowski', p=2):\n",
    "    model = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, metric=metric, p=p, n_jobs=10)\n",
    "\n",
    "    \"\"\" train \"\"\"\n",
    "    y_pred = model.fit_predict(x)\n",
    "\n",
    "    \"\"\" evaluate \"\"\"\n",
    "    print('Evaluate on test set')\n",
    "    roc_auc = evaluate(y, y_pred)\n",
    "\n",
    "    # scores = model.score_samples(x_test)\n",
    "    scores = model.negative_outlier_factor_\n",
    "\n",
    "    del x\n",
    "    # del y_test_pred, y_train_pred\n",
    "    # del model\n",
    "\n",
    "    return model, y_pred, roc_auc, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 01: Run on original features (0-8)\n",
    "\n",
    "`Dur`, `sTos`, `dTos`, `Sport`, `Dport`, `TotPkts`, `TotBytes`, `SrcBytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dur, sTos, dTos, Sport, Dport, PktsPerSec, BytesPerSec, SrcBytesPerSec'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" select features \"\"\"\n",
    "selected_fts_names__01 = fts_names[0:8]\n",
    "', '.join(selected_fts_names__01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301637, 63)\n"
     ]
    }
   ],
   "source": [
    "X__01 = Xval[:,:]\n",
    "\n",
    "print(X__01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set\n",
      "[[448659   4465]\n",
      " [ 11326    182]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9754    0.9901    0.9827    453124\n",
      "           1     0.0392    0.0158    0.0225     11508\n",
      "\n",
      "    accuracy                         0.9660    464632\n",
      "   macro avg     0.5073    0.5030    0.5026    464632\n",
      "weighted avg     0.9522    0.9660    0.9589    464632\n",
      "\n",
      "0.5029806351541762\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 40\n",
    "contamination = 0.01\n",
    "model__01, y_pred__01, roc_auc__01, scores__01 = test_param(X__01, y, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "save_result(model__01, X__01, y_pred__01, scores__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set\n",
      "[[292276   2812]\n",
      " [  6344    205]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9788    0.9905    0.9846    295088\n",
      "           1     0.0679    0.0313    0.0429      6549\n",
      "\n",
      "    accuracy                         0.9696    301637\n",
      "   macro avg     0.5234    0.5109    0.5137    301637\n",
      "weighted avg     0.9590    0.9696    0.9641    301637\n",
      "\n",
      "0.510886564098269\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 40\n",
    "contamination = 0.01\n",
    "model__01, y_pred__01, roc_auc__01, scores__01 = test_param(X__01, y, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "save_result(model__01, X__01, y_pred__01, scores__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set\n",
      "[[292205   2883]\n",
      " [  6415    134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9785    0.9902    0.9843    295088\n",
      "           1     0.0444    0.0205    0.0280      6549\n",
      "\n",
      "    accuracy                         0.9692    301637\n",
      "   macro avg     0.5115    0.5053    0.5062    301637\n",
      "weighted avg     0.9582    0.9692    0.9636    301637\n",
      "\n",
      "0.5053455860900432\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 20\n",
    "contamination = 0.01\n",
    "model__01, y_pred__01, roc_auc__01, scores__01 = test_param(X__01, y, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "save_result(model__01, X__01, y_pred__01, scores__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set\n",
      "[[126090   1258]\n",
      " [  1867     75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9854    0.9901    0.9878    127348\n",
      "           1     0.0563    0.0386    0.0458      1942\n",
      "\n",
      "    accuracy                         0.9758    129290\n",
      "   macro avg     0.5208    0.5144    0.5168    129290\n",
      "weighted avg     0.9715    0.9758    0.9736    129290\n",
      "\n",
      "0.5143707680410065\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 30\n",
    "contamination = 0.01\n",
    "model__01, y_pred__01, roc_auc__01, scores__01 = test_param(X__01, y, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "save_result(model__01, X__01, y_pred__01, scores__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set\n",
      "[[126142   1206]\n",
      " [  1862     80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9855    0.9905    0.9880    127348\n",
      "           1     0.0622    0.0412    0.0496      1942\n",
      "\n",
      "    accuracy                         0.9763    129290\n",
      "   macro avg     0.5238    0.5159    0.5188    129290\n",
      "weighted avg     0.9716    0.9763    0.9739    129290\n",
      "\n",
      "0.5158622656530544\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 50\n",
    "contamination = 0.01\n",
    "model__01, y_pred__01, roc_auc__01, scores__01 = test_param(X__01, y, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "save_result(model__01, X__01, y_pred__01, scores__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set\n",
      "[[126120   1228]\n",
      " [  1858     84]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9855    0.9904    0.9879    127348\n",
      "           1     0.0640    0.0433    0.0516      1942\n",
      "\n",
      "    accuracy                         0.9761    129290\n",
      "   macro avg     0.5248    0.5168    0.5198    129290\n",
      "weighted avg     0.9716    0.9761    0.9739    129290\n",
      "\n",
      "0.5168057542851433\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 60\n",
    "contamination = 0.01\n",
    "model__01, y_pred__01, roc_auc__01, scores__01 = test_param(X__01, y, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "save_result(model__01, X__01, y_pred__01, scores__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set\n",
      "[[126642    706]\n",
      " [  1896     46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9852    0.9945    0.9898    127348\n",
      "           1     0.0612    0.0237    0.0341      1942\n",
      "\n",
      "    accuracy                         0.9799    129290\n",
      "   macro avg     0.5232    0.5091    0.5120    129290\n",
      "weighted avg     0.9714    0.9799    0.9755    129290\n",
      "\n",
      "0.5090715283213829\n"
     ]
    }
   ],
   "source": [
    "# n_neighbors = 60\n",
    "# contamination = 0.006\n",
    "# model__01, lof_outlier_factor__01, y_test_pred__01, roc_auc__01 = test_param(X_train__01, X_test__01, y_train, y_test, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "# save_result(model__01, X_train__01, X_test__01, y_test_pred__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 70\n",
    "contamination = 0.01\n",
    "model__01, y_pred__01, roc_auc__01, scores__01 = test_param(X__01, y, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "save_result(model__01, X__01, y_pred__01, scores__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test set\n",
      "[[126065   1283]\n",
      " [  1862     80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9854    0.9899    0.9877    127348\n",
      "           1     0.0587    0.0412    0.0484      1942\n",
      "\n",
      "    accuracy                         0.9757    129290\n",
      "   macro avg     0.5221    0.5156    0.5180    129290\n",
      "weighted avg     0.9715    0.9757    0.9736    129290\n",
      "\n",
      "0.515559944454449\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 80\n",
    "contamination = 0.01\n",
    "model__01, y_pred__01, roc_auc__01, scores__01 = test_param(X__01, y, n_neighbors=n_neighbors, contamination=contamination)\n",
    "\n",
    "save_result(model__01, X__01, y_pred__01, scores__01, 'lof', selected_fts_names__01, roc_auc__01, 'exp01_fts-0-8', f'n_neighbors={n_neighbors}\\ncontamination={contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train__01, X_test__01"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
